# Ollama Chat

## Introduction
Welcome to Ollama Chat, a web app built with Vue that enables users to chat with local Language Learning Models (LLMs) offline using the Ollama API.

## Features
- Offline chat functionality with local LLMs
- Integration with Ollama API endpoints
- User-friendly interface for easy conversation

## Getting Started
1. Clone the repository: `git clone (link unavailable)`
2. Install dependencies: `npm install`
3. Start the app: `npm run dev` or `npm run build` to compile the app to run without serving with vite
4. Open the app in your browser: [http://localhost:5173](http://localhost:5173) or your dedicated Vite port

## Usage
1. Select a model from the dropdown menu
2. Type your message in the input field
3. Click the "Send" button to send the message
4. View the response from the LLM in the chat window

## Configuration
- API endpoint URL: `[http://localhost:11434/api](http://localhost:11434/api)`


## Troubleshooting
- Check the console logs for errors
- Verify that the API endpoint URL and API key are correct

## License
This project is licensed under the MIT License.

## Acknowledgments
- Ollama API for providing the language learning models
- Vue.js for the web app framework

## Contact
- Email: 
- GitHub: ([Richprince23](https://github.com/richprince23))


## Recommended IDE Setup

[VSCode](https://code.visualstudio.com/) + [Volar](https://marketplace.visualstudio.com/items?itemName=Vue.volar) (and disable Vetur).

## Customize configuration

See [Vite Configuration Reference](https://vitejs.dev/config/).

## Project Setup

```sh
npm install
```

### Compile and Hot-Reload for Development

```sh
npm run dev
```

### Compile and Minify for Production

```sh
npm run build
```
